{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# for visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# For Hypothesis testing\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# For principal components analysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset into pandas dataframe\n",
    "path = \"../data/census-income.data.gz\"\n",
    "# set the column names\n",
    "censusColnames = ['Age', 'ClassOfWorker', 'Industry', 'Occupation', 'Education',\n",
    "                  'WagePerHr', 'EducationalInst', 'MaritalStatus', 'IndustryCode', \n",
    "                  'OccupationCode', 'Race', 'HispanicOrigin', 'Sex', 'MemLabourUnion',\n",
    "                  'UnemploymentReason', 'EmploymentStatus', 'CapitalGain', 'CapitalLoss',\n",
    "                  'Dividends', 'FEDERALTAX', 'TaxFilerStat', 'PrevState', \n",
    "                  'HouseholdStatus', 'HouseholdSummary', 'INSTANCEWEIGHT', \n",
    "                  'MigrationCode_MSA', 'MigrationCode_REG', \n",
    "                  'MigrationCode_WITHIN_REG', 'HouseOneYearAgo', \n",
    "                  'MigrationPrevResInSunbelt', 'NumOfPersonForEmployer', 'Parent', \n",
    "                  'BirthCountryFather', 'BirthCountryMother', 'BirthCountrySelf', \n",
    "                  'Citizenship', 'OwnBusiness', 'VeteranQA', 'VeteranBenefits', \n",
    "                  'WeeksWorked', 'Year', 'targetIncome']\n",
    "censusDf = pd.read_csv(path, sep=r',', skipinitialspace=True, \n",
    "                       names = censusColnames, header='infer')\n",
    "\n",
    "# Printing the dimensions of the dataset\n",
    "print(censusDf.shape[0],\"rows,\", censusDf.shape[1],\"columns\")\n",
    "\n",
    "# Displaying first five elements of all columns\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(censusDf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">From the various features in the census data set our aim is to build a predictive model to determine whether the income level for the people in United States exceeds the bracket of $50,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our problem statement is clear that it is a binary classification problem.\n",
    "\n",
    "Let us generate some hypotheses which will help us in building the models more efficiently. We need to figure out some hypotheses which might influence our final outcome, hence we need to answer a simple question.\n",
    "\n",
    "**Is There a Relationship Between the Response and Predictors?**\n",
    "\n",
    "To test this we use the test between the Null Hypothesis $H_0$ versus the Alternate Hypothesis $H_a$.\n",
    "* $H_0$ : There is no relationship between the response Income and the predictors.\n",
    "    * To test the Null Hypothesis we test whether all the regression coefficients are zero.\n",
    "* $H_a$ : There is some realtionship between the response and the predictors.\n",
    "    * To test the Alternate Hypothesis we find  at least one coefficient that is non-zero.\n",
    "    \n",
    "*To perform the Hypothesis tests we will be performing multivariate linear regression on ordinal values of the dataset using **statsmodels** library.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for hypothesis testing\n",
    "censusDf_htest = censusDf\n",
    "censusDf_htest['targetIncome'] = pd.get_dummies(censusDf_htest.targetIncome).iloc[:,1:]\n",
    "\n",
    "# Constructing a linearmodel using the ordinal values for our initial hypothesis test\n",
    "hypothesis_test_model = smf.ols(formula=(\"targetIncome ~ Age + Industry + Occupation + \"\n",
    "             \"WagePerHr + CapitalGain + CapitalLoss + Dividends + \"\n",
    "             \"INSTANCEWEIGHT + NumOfPersonForEmployer + OwnBusiness +\"\n",
    "             \"VeteranBenefits + WeeksWorked + Year\"), data=censusDf_htest).fit()\n",
    "\n",
    "# Printing the summary of the model\n",
    "hypothesis_test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above result that none of the coefficients are zero, also some of the features have significant p-values, which indicates that there is a significant relationship among the predictors and the response. \n",
    "\n",
    "* Hence we reject our Null Hypothesis $H_0$.\n",
    "\n",
    "We should keep in mind that we have not considered all the features for our hypothesis generation, we will explore more about the nominal features as we proceed in the coming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our model we should define some baseline. Let us generate some statistics about our response variable so that we can set our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the count\n",
    "incomeCount = censusDf['targetIncome'].value_counts()\n",
    "print(incomeCount)\n",
    "\n",
    "# Getting the proportion of data having -50000 as response\n",
    "print(float(incomeCount[0]/len(censusDf['targetIncome']))*100,\n",
    "     \"% people have income below $50000.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the values are 0 in the responce variable, Income. Which means that the dataset is heavily skewed towards having income less than \\$50,000. Which means that if we predict only below \\$50,000, still our model accuracy would be **93.79%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censusDf.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can observe from the above statistics that, there are no missing values in numerical columns of the dataset. \n",
    "* There is only one column in which there are 874 missing values, which is 'HispanicOrigin'.\n",
    "* From the first five lines of dataframe displayed above we saw that there are some garbage/missing values in the dataframe labelled as '?', lets try to track them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are lot of '?' appearing in the dataset lets track them\n",
    "for i in censusDf.columns:\n",
    "    if '?' in list(censusDf[i]):\n",
    "        print(censusDf.loc[censusDf[i].isin(['?'])][i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above missing values does not makes much sense if we substitute them, as they are nominal values. Let us label all the above missing values as 'Unavailable'. Also there are four columns in which there almost 50% of the values which are '?', it is better to drop those columns, as high proportion of missing values can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns with missing values more than 50% and storing in a new dataframe\n",
    "censusDf_cleaned = censusDf.drop(['MigrationCode_MSA', 'MigrationCode_REG', \n",
    "                                  'MigrationCode_WITHIN_REG', \n",
    "                                  'MigrationPrevResInSunbelt'], axis=1)\n",
    "\n",
    "# Replacing the '?' with the label 'Unavailable'\n",
    "censusDf_cleaned = censusDf_cleaned.replace('?', 'Unavailable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the values are replaced\n",
    "for i in censusDf_cleaned.columns:\n",
    "    if 'Unavailable' in list(censusDf_cleaned[i]):\n",
    "        print(censusDf_cleaned.loc[censusDf_cleaned[i].isin(['Unavailable'])][i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we saw earlier, for the caolumn 'HispanicOrigin' we have few (874) missing values; lets see how the values are distributed in the column, so that we can impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "censusDf_cleaned['HispanicOrigin'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column for the missing values for HispanicOrigin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the missing value in a variable\n",
    "missing_val = censusDf_cleaned[censusDf_cleaned.isnull()]['HispanicOrigin'].iloc[1]\n",
    "# impute the missing values\n",
    "censusDf_cleaned['HispanicOrigin'] = censusDf_cleaned['HispanicOrigin'].replace(\n",
    "    missing_val, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the values are replaced\n",
    "censusDf_cleaned['HispanicOrigin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Check for missing values one last time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "censusDf_cleaned.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing the columns\n",
    "\n",
    "# Replacing the 'targetIncome' values with dummy variables\n",
    "# - 50000. as the baseline. 0 for - 50000. and 1 for 50000+.\n",
    "censusDf_cleaned['targetIncome'] = pd.get_dummies(\n",
    "    censusDf_cleaned.targetIncome).iloc[:,1:]\n",
    "\n",
    "# Features and Outcome\n",
    "X = censusDf_cleaned.drop('targetIncome',1)\n",
    "y = censusDf_cleaned.targetIncome\n",
    "print(\"X (predictors) is \",X.shape[0],\"rows,\", X.shape[1],\"columns, and...\"\\\n",
    "      \"\\ny (response) is \",y.shape[0],\"rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Let us check the categorical variables in for each feature, and decide which one to  use in our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print out number of unique categorical values in each column\n",
    "print(\"NUMBER OF UNIQUE VALUES IN EACH FEATURE:\\n\")\n",
    "for col_name in X.columns:\n",
    "    if X[col_name].dtype == 'object':\n",
    "        unique_val = len(X[col_name].unique())\n",
    "        print(\"'{col_name}' has --> {unique_val}\\\n",
    "        \".format(col_name=col_name, unique_val=unique_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It looks like the columns 'BirthCountryFather', 'BirthCountryMother' and 'BirthCountrySelf' have same number of unique values. Let us keep only one column, and drop the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns\n",
    "X = X.drop(['BirthCountryFather', 'BirthCountryMother'], axis=1)\n",
    "# keeping 'BirthCountrySelf' and renaming\n",
    "X.rename(columns={'BirthCountrySelf': 'BirthCountry'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Although, 'BirthCountry' has a lot of unique categories, ...\n",
    "# ...most categories only have a few observations if compared to max (United-States)\n",
    "X['BirthCountry'].value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, bucket low frequecy categories as \"Other\"\n",
    "X['BirthCountry'] = ['United-States' if x == 'United-States' \n",
    "                       else 'Other-Countries' for x in X['BirthCountry']]\n",
    "# check the values\n",
    "X['BirthCountry'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The column 'HouseholdStatus' has 38 unique values; only few of the categories have significant number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the value counts\n",
    "X['HouseholdStatus'].value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to categorize the values as other, which does not have significant count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket the low frequency category as other\n",
    "X['HouseholdStatus'] = ['Householder' if x == 'Householder'\n",
    "                        else 'Children' if x == 'Child <18 never marr not in subfamily'\n",
    "                        else 'Spouse' if x == 'Spouse of householder'\n",
    "                        else 'Nonfamily' if x == 'Nonfamily householder'\n",
    "                        else 'Child_18_plus' if x == 'Child 18+ never marr Not in a subfamily'\n",
    "                        else 'Secondary_indv' if x == 'Secondary individual'\n",
    "                       else 'Other_Householders' for x in X['HouseholdStatus']]\n",
    "# check the values\n",
    "X['HouseholdStatus'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets check the 'PrevState' column, there are 51, unique values for the feature, lets see what are they."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts\n",
    "X['PrevState'].value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With approximately 200,000 rows in our dataset, there are almost 184,000 values for the 'PrevState' column, that say 'Not in universe', which is almost 96% of the entire row, since the survey has been conducted in the United States of America, all of them must belong to a state, hence the value stating \"Not in universe\" are the missing values. Having this much small information about the sate doesn't seem to be helpful, it is better that we drop this feature from our predictors variables list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'PrevState' column\n",
    "X = X.drop(['PrevState'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coverting categorical variable in to _Dummy Variables_.** If we want to include a categorical feature in our machine learning model, one common solution is to create dummy variables. We drop the original feature from the dataset and add a dummied version of the feature to the dataset, which is easier for the model to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of categorical features to create a dummy variable of\n",
    "# columns names in asscending order, according to number of diff unique values\n",
    "features_to_dummy = ['Sex', 'BirthCountry', 'Year', 'EducationalInst', \n",
    "                     'MemLabourUnion', 'HouseOneYearAgo', 'OwnBusiness', 'VeteranQA',\n",
    "                     'VeteranBenefits', 'Race', 'Parent', 'Citizenship', \n",
    "                     'UnemploymentReason', 'FEDERALTAX', 'TaxFilerStat', \n",
    "                     'MaritalStatus', 'HouseholdStatus', 'NumOfPersonForEmployer', \n",
    "                     'EmploymentStatus', 'HouseholdSummary', 'ClassOfWorker', \n",
    "                     'HispanicOrigin', 'OccupationCode', 'Education', \n",
    "                     'IndustryCode', 'Occupation', 'Industry','WeeksWorked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create dummy variables of the dataframe from the list of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the dummy categorical variables used for modeling\n",
    "def create_dummies(df, col_name_list):\n",
    "    \"\"\"\n",
    "    This function takes the dataframe and features list as input, \n",
    "    and returns the modified dataframe with dummy variables of the \n",
    "    features in the list col_name_list.\n",
    "    \n",
    "    :param df: target dataframe \n",
    "    :param col_name_list: list of the column names from the dataset\n",
    "    :return: modifies the dataframe df inplace and returns dummied dataframe\n",
    "             of features in col_name_list\n",
    "    \"\"\"\n",
    "    for x in col_name_list:\n",
    "        dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False)\n",
    "        df = df.drop(x, 1)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function create_dummies to convert our features in to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before dummies\n",
    "print(\"Dataframe X has\", X.shape[1],\"columns\",X.shape[0],\"and rows.\")\n",
    "\n",
    "# Call the function create_dummies on X and replace the features with dummies\n",
    "print(\"Creating dummies ...\")\n",
    "X = create_dummies(X, features_to_dummy)\n",
    "\n",
    "# Printing the dimensions of the modified feature set\n",
    "print(\"*** Now our dataframe has\", X.shape[1],\"columns\",X.shape[0],\"and rows. ***\")\n",
    "\n",
    "# display first five rows of all the features\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) transforms the dataset of many features into few Principal Components that \"summarize\" the variance underying in the data. It is the  most common way of dimensionality reduction, and it works well where the features are highly corelated. The drawback of using PCA is that it makes it difficult to interpret the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use PCA from sklearn.decomposition to find the principal components\n",
    "pca = PCA(n_components=10) # 10 principal components\n",
    "X_pca = pd.DataFrame(pca.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of 10 pcs\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this case we will not proceed with the principal components. Because it is not recommended to perform PCA on categorical data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetrics(object):\n",
    "\n",
    "    # Random permutation cross-validator with 80-20 train test split\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state=1)\n",
    "    # dictionary to store scores\n",
    "    model_scores = dict()\n",
    "    # default scoring metrics\n",
    "    default_metric = ['accuracy','precision', 'recall']\n",
    "    \n",
    "    def __init__(self, model_name, model_obj, features, response):\n",
    "        self.model_name = model_name\n",
    "        self.model_obj = model_obj\n",
    "        ModelMetrics.model_scores[model_name] = []\n",
    "        self.cv = ModelMetrics.cv\n",
    "        self.features = features\n",
    "        self.response = response\n",
    "        self.model_scores = ModelMetrics.model_scores\n",
    "        \n",
    "    def model_scoring(self, scoring_metric=default_metric):\n",
    "        for metric in scoring_metric:\n",
    "            n_fold_score = cross_val_score(self.model_obj,self.features,\n",
    "                                                           self.response,\n",
    "                                                           cv=self.cv,\n",
    "                                                           scoring=metric)\n",
    "            self.model_scores[self.model_name].append({metric:n_fold_score})\n",
    "        model_scores = self.model_scores\n",
    "        return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metric_df(metric_dict):\n",
    "    \"\"\"takes input as dict obtained from ModelMetrics\n",
    "    class and maps the scores to a pandas dataframe\n",
    "    :params: \n",
    "    metric_dict : dictionary of scores \n",
    "    :returns:\n",
    "    score_df : pandas dataframe mapping the metrics and model\n",
    "    \"\"\"\n",
    "    \n",
    "    # use for loop to store the values\n",
    "    model_name, acc_nfolds, pre_nfolds, rec_nfolds = (list() for i in range(4)) \n",
    "    for model, metrics in metric_dict.items():\n",
    "        model_name.append(model)\n",
    "        acc_nfolds.append(list(metrics[0].values())[0])\n",
    "        pre_nfolds.append(list(metrics[1].values())[0]) \n",
    "        rec_nfolds.append(list(metrics[2].values())[0])\n",
    "    \n",
    "    metric_col_names = ['accuracy', 'precision', 'recall',\n",
    "                'accuracy_nfolds', 'precision_nfolds', 'recall_nfolds']\n",
    "    score_df = pd.DataFrame(columns = metric_col_names, index=model_name)\n",
    "\n",
    "    # assign values\n",
    "    score_df['accuracy_nfolds'] = acc_nfolds\n",
    "    score_df['precision_nfolds'] = pre_nfolds\n",
    "    score_df['recall_nfolds'] = rec_nfolds\n",
    "    score_df['accuracy'] = score_df['accuracy_nfolds'].apply(np.mean)\n",
    "    score_df['precision'] = score_df['precision_nfolds'].apply(np.mean)\n",
    "    score_df['recall'] = score_df['recall_nfolds'].apply(np.mean)\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_each_fold(metric, title):\n",
    "    metric = metric+\"_nfolds\"\n",
    "    legend_list = list()\n",
    "    for i,model in enumerate(list(final_results.index)):\n",
    "        x_val = list(\"fold-\"+str(i) \n",
    "                     for i in range(len(final_results[metric][0])))\n",
    "        y_val = final_results[metric][i]\n",
    "        \n",
    "        # fig size\n",
    "        plt.rcParams[\"figure.figsize\"] = (16, 9)\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        plt.plot(x_val,y_val,\n",
    "                marker=\".\", markeredgewidth=1,linestyle=\":\", linewidth=3.5)\n",
    "        for a,b in zip(x_val,y_val): \n",
    "            plt.text(a, b, str(b))\n",
    "        legend_list.append(model)\n",
    "        plt.legend(legend_list)\n",
    "        plt.title(title)\n",
    "\n",
    "    # add a horiontal line for baseline\n",
    "    if(metric == \"accuracy_nfolds\"):\n",
    "        baseline = float(y.value_counts()[0]/len(y))\n",
    "        plt.axhline(y=baseline, color='r', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing dataset for testing model scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = X.head(20000)\n",
    "Y_small = y.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier implementing Logistic Regression\n",
    "clf_log_reg = LogisticRegression()\n",
    "# Creating object for metrics\n",
    "log_reg_metrics_obj = ModelMetrics(\"Logistic Regression\", clf_log_reg, X_small, Y_small)\n",
    "# get the metrics\n",
    "log_reg_metrics = log_reg_metrics_obj.model_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier implementing the k-nearest neighbors\n",
    "clf_knn = KNeighborsClassifier()\n",
    "# Creating object for metrics\n",
    "clf_knn_metrics_obj = ModelMetrics(\"k-nearest neighbors\", clf_knn, X_small, Y_small)\n",
    "# get the metrics\n",
    "clf_knn_metrics = clf_knn_metrics_obj.model_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier implementing the Decision Tree\n",
    "clf_d_tree = DecisionTreeClassifier()\n",
    "# Creating object for metrics\n",
    "clf_d_tree_metrics_obj = ModelMetrics(\"Decision Tree\", clf_d_tree, X_small, Y_small)\n",
    "# get the metrics\n",
    "clf_d_tree_metrics = clf_d_tree_metrics_obj.model_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier implementing  Random Forest Classifier\n",
    "clf_RF = RandomForestClassifier()\n",
    "# Creating object for metrics\n",
    "clf_RF_metrics_obj = ModelMetrics(\"Random Forest Classifier\", \n",
    "                                      clf_RF, X_small, Y_small)\n",
    "# get the metrics\n",
    "clf_RF_metrics = clf_RF_metrics_obj.model_scoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the results in a DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sotore the results in a data frame\n",
    "final_results = create_metric_df(ModelMetrics.model_scores)\n",
    "# print the average score for all folds\n",
    "final_results.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores for n folds\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "final_results.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"accuracy\"\n",
    "title = \"Compare Accuracy of all the models at each fold\"\n",
    "plot_metric_each_fold(metric, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"precision\"\n",
    "title = \"Compare Precision of all the models at each fold\"\n",
    "plot_metric_each_fold(metric, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"recall\"\n",
    "title = \"Compare Recall of all the models at each fold\"\n",
    "plot_metric_each_fold(metric, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
